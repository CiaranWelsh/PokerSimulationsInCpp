//
// Created by Ciaran Welsh on 02/02/2021.
//

#ifndef POKERX_POLICY_H
#define POKERX_POLICY_H


#include <memory>
#include <vector>
#include "PokerX/engine/Action.h"

namespace pokerx {


    /**
     * The idea behind a policy comes from reinforcement learning
     * whereby a learning agent learns the best policy. The agent
     * follows its best policy or explores new territory in attempt to
     * improve its policy. This is too much detail for now.
     *
     * The Policy for now only takes sequences of actions that
     * a player will take. At the moment the call station player
     * has a constant policy of calling all of the time. However,
     * we want to enable the situation where a player can deterministically
     * take a series of action, like check, call, raise. This will massively
     * help in building and testing game scenarios.
     *
     * In a way this idea is already wrapped up in the player. I like the
     * idea of making it more explicit by having a separate entity for it. This
     * means that we can assign a player a policy, which actually is a bit like
     * the current mechanism of inheriting from Player. Are they equivalent?
     *
     * Should the Policy be a GameVariables Observer? I suspect so. At the moment
     * my primary interest is passing sequences of actions to players for them to take
     * in sequential turns, but eventually these should be based on the game variables
     * as well as contributions to the pot by other players.
     *
     * Having a vector of actions as input to the Policy makes sense for now.
     * The Policy could observe the whole PokerEngine. Actually it should, since it
     * needs both player and game information. Is there any advantage of creating an observer
     * compared to just passing in a pointer to the PokerEngine? Or a policy could
     * be assigned to a Player which already has access to game variables by the current
     * observer mechanism. So we could just bind a policy to a player.
     *
     * Also, Policies need to be generated on the fly, as new information comes in.
     *
     * Different policies for different situations that are generated by AI models
     * as new information comes in
     *
     *
     *
     *
     *
     *
     * It'd be good to start training from actual player information.
     * We could play each game from the perspective of each player
     * and train a player on what kinds of situations can happen
     * and which are profitable. From 1M hands, we can only play
     * from the perspective of those cards we have seen in the history,
     * so probs about 20%
     *
     * We can also calculate features based on the game, like pot odds
     * and equity.
     *
     * the reward is the difference between the start and end stack for
     * each poker hand. In these hands, we can take every players line
     * whos hole cards we know and train a regression algorithm. The y
     * value is the amount gained or lost for each hand. Actually,
     * we should also follow a players whole session... Or we could
     * just try and take the fully RL way of doing things?
     *
     * The above shows that i don't know what the requirements are
     * for this class. therefore, I'll implement what I need right now
     * and change it when I figure out what needs doin.
     */
    class Policy {

    public:
        Policy() = default;

        explicit Policy(std::vector<Action> actions);

        explicit Policy(std::vector<Action>  actions, std::vector<float>  raiseAmounts);

        [[nodiscard]] const std::vector<Action> &getActions() const;

        void setActions(const std::vector<Action> &actions);

        Action nextAction();

        void setNextAction(int nextAction);

        [[nodiscard]] const std::vector<float> &getRaiseAmounts() const;

        void setRaiseAmounts(const std::vector<float> &raiseAmounts);

    private:

        std::vector<Action> actions_;

        std::vector<float> raiseAmounts_;

        int nextAction_ = 0;

    };

}

#endif //POKERX_POLICY_H
